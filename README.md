# what-I-read-2023
Name of papers what I've read in 2023

## February
### Main Interest : Time Series Pretraining

TS-BERT : Time Series Anomaly Detection via Pre-training Model BERT (ICCS 2021)

Tabular Transformers for modeling multivariate time series (ArXiv 2020)

A transformer-based framework for multivariate time series representation learning (KDD 2021)

Attention Augmented Convolutional Transformer for Tabular TIme-Series (ICDMW 2021)

MAD : Self-Supervised Masked Anomaly Detection Task for Multivariate Time Series (ArXiv 2022) 

Time Series Generation with Masked Autoencoder (Arxiv 2022)

Should you Mask 15% in Masked Language Modeling? (ArXiv, 2022)

Masked Language Modeling and the Distributional Hypothesis:Order Word Matters Pre-training for Little (EMNLP 2021)

data2vec : A general Framework for self-supervised learning in speech, vision, and language (ArXiv 2022)
