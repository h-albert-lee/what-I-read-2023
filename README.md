# what-I-read-2023
Name of papers I've read in 2023

## January + February
### Main Interest : Time Series Pretraining, LOB

TS-BERT : Time Series Anomaly Detection via Pre-training Model BERT (ICCS 2021)

Tabular Transformers for modeling multivariate time series (ArXiv 2020)

A transformer-based framework for multivariate time series representation learning (KDD 2021)

Attention Augmented Convolutional Transformer for Tabular TIme-Series (ICDMW 2021)

MAD : Self-Supervised Masked Anomaly Detection Task for Multivariate Time Series (ArXiv 2022) 

Time Series Generation with Masked Autoencoder (Arxiv 2022)

Should you Mask 15% in Masked Language Modeling? (ArXiv, 2022)

Masked Language Modeling and the Distributional Hypothesis:Order Word Matters Pre-training for Little (EMNLP 2021)

data2vec : A general Framework for self-supervised learning in speech, vision, and language (ArXiv 2022)

Transformers for limit order books (ArXiv 2020)

Deep Learning Modelling of the limit order book : a comparative perspective (ArXiv 2020)

Decision Transformer : Reinforcement Learning via Sequence Modeling (ArXiv 2021)

Deep Learning Book : Ch.15 Representation Learning (MIT Press, 2016)

Offline Reinforcement learning as One Big Sequence Modeling Problem (Neurips 2021)

PrimeNet:Pre-Training for Irregular Multivariate Time Series (ArXiv 2023)

Learning to simulate realistic limit order book markets grom data as a World Agent (ArXiv 2022)

Forecasting Quoted Depth with the Limit Order Book (Journal of frontiers in artificial intelligence 2021)

Multi-Horizon Forecasting for Limit Order Books: Novel Deep Learning Approaches and Hardware Acceleration using Intelligent Processing units(ArXiv 2021)

Effects of Limit Order Book Information Level on Market Stability Metrics (Journal of Economic Interaction and Coordination 2015)

Intraday Seasonalities and Nonstationarity of Trading Volume in Financial Markets : Individual and Cross-Sectional Features (PLoS ONE 2016)

Intraday Volume Percentages Forecasting Usinga Dynamic SVM-based Approach (Journal of system science and complexity 2017)

Volume Prediction With Neural Networks (Journal of frontiers in artificial intelligence 2019)

Predicting Daily Trading Volume via Various Hidden states (ArXiv 2021)

## March
### Main Interest : Multimodal learning, Encoder-Decoder models

Self-Supervised Multimodal Opinion Summarization (ArXiv 2021)

Are Transformers Effective for Time Series Forecasting? (ArXiv 2022)

Longformer: The Long-Document Transformer(ArXiv 2020)

Does Synthetic Data Generation of LLMs Help Clinical Text Mining?(ArXiv 2023)

Language is not all you need : Aligning perception with language models (ArXiv 2023)

ChatGPT outperforms crowd-workers for text-annotation tasks (ArXiv 2023)

## April
### Main Interest : Multimodal learning, Large Language Models 

A sequence to sequence transformer data logic experiment (ACL 2021)

No Stock is an Island : Learning Internal and Relational Attributes of Stocks with Contrastive Learning (ACL 2022)

LLaMA: Open and Efficient Foundation Language Models (ArXiv 2023)

Scaling Instruction-Finetuned language models (ArXiv 2022)

Training language models to follow instructions with human feedback (ArXiv 2022)

GPT3Mix: Leveraging Large-Scale Language Model for Text Augmentation (EMNLP 2021)

## May
### Main Interest : Large Language Models, Instruction Tuning

Can Chatgpt Forecast Stock Price Movements? Return Predictability and Large lanugage Models (ArXiv 2023)

Pangu-α: Large-scale autoregressive pretrained chinese language models with auto-parallel computation (ArXiv 2021)

CPM-2: large-scale cost-effective pre-trained language models (ArXiv 2021)

GLM-130B: an open bilingual pre-trained model (ArXiv 2022)

BLOOM: A 176b-parameter open-access multilingual language model (ArXiv 2022)

ERNIE 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation (ArXiv 2021)

Training compute-optimal large language models (ArXiv 2022)

Every Author as First Author (ArXiv 2023)

Sparks of Artificial General Intelligence : Early Experiemnts with GPT-4 (ArXiv 2023)

## June
### Main Interest : Large Language Models, Instruction Tuning

The Flan Collection: Designing Data and Methods for Effective Instruction Tuning (ArXiv 2023)

Emergent Abilities of Large Language Models (Transactions on Machine Learning Research 2022)

Faith and Fate: Limits of Transformers on Compositionality(ArXiv 2023)

Frequency of Geographical Names and Sentiment Analysis of Eighteenth and Nineteenth Century English Novels through Moretti’s “Distant Reading” (KOAJ 2021)

FinGPT: Open-Source Financial Large Language Models (ArXiv 2023)

Language Models Can See: Plugging Visual Controls in Text Generation (ArXiv 2022)

LIMA: Less Is More for Alignment (ArXiv 2023)

Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis (ICML 2023)

Textbooks are all you need (ArXiv 2023)

## July
### Main Interest : Model Parallelism, Evaluation of Large Language Models

Model-Agnostic Meta Learning for Natural Language Understanding task in finance (IJCAI 2023)

Inverse Scaling : When Bigger isn't Better (ArXiv 2023)

LLAMA 2: Open Foundation and Fine-Tuned Chat Models (Meta Blog 2023)

How is ChatGPT's behavior changing over time? (ArXiv 2023)

Communicative Agents for Software Development (ArXiv 2023)

Open-WikiTable: Dataset for Open Domain Question Answering with Complex Reasoning over Table (ArXiv 2023)

## August
### Main Interest : Model Parallelism, Evaluation of Large Language Models

A Survey on Evaluation of Large Language Models (ArXiv 2023)

A Survey of Large Language Models (ArXiv 2023)

FinEval : A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models (ArXiv 2023)

Consciousness in Artificial Intelligence : Insights from the Science of Consciousness (ArXiv 2023)

## September
### Main Interest : Instruction Tuning, Evaluation of Large Language Models

Instruction Tuning for Large Language Models : A Survey (ArXiv 2023)

Large Language Models as Optimizers (ArXiv 2023)

Best humans still outperform artificial intelligence in acreative divergent thinking task (Nature 2023)
