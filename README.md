# what-I-read-2023
Name of papers what I've read in 2023

## February
### Main Interest : Time Series Pretraining, LOB

TS-BERT : Time Series Anomaly Detection via Pre-training Model BERT (ICCS 2021)

Tabular Transformers for modeling multivariate time series (ArXiv 2020)

A transformer-based framework for multivariate time series representation learning (KDD 2021)

Attention Augmented Convolutional Transformer for Tabular TIme-Series (ICDMW 2021)

MAD : Self-Supervised Masked Anomaly Detection Task for Multivariate Time Series (ArXiv 2022) 

Time Series Generation with Masked Autoencoder (Arxiv 2022)

Should you Mask 15% in Masked Language Modeling? (ArXiv, 2022)

Masked Language Modeling and the Distributional Hypothesis:Order Word Matters Pre-training for Little (EMNLP 2021)

data2vec : A general Framework for self-supervised learning in speech, vision, and language (ArXiv 2022)

Transformers for limit order books (ArXiv 2020)

Deep Learning Modelling of the limit order book : a comparative perspective (ArXiv 2020)

Decision Transformer : Reinforcement Learning via Sequence Modeling (ArXiv 2021)

Deep Learning Book : Ch.15 Representation Learning (MIT Press, 2016)

Offline Reinforcement learning as One Big Sequence Modeling Problem (Neurips 2021)

PrimeNet:Pre-Training for Irregular Multivariate Time Series (ArXiv 2023)

Learning to simulate realistic limit order book markets grom data as a World Agent (ArXiv 2022)

Forecasting Quoted Depth with the Limit Order Book (the journal of frontiers in artificial intelligence 2021)

Multi-Horizon Forecasting for Limit Order Books: Novel Deep Learning Approaches and Hardware Acceleration using Intelligent Processing units(ArXiv 2021)
